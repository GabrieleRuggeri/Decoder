{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679c7015",
   "metadata": {},
   "source": [
    "Tiny Code Challenge ðŸ’»\n",
    "\n",
    "Can you try to write a small snippet (in NumPy or PyTorch, whichever you like) that generates this n x n causal mask given a sequence length n?\n",
    "\n",
    "ðŸ‘‰ Try it for n=5 and check if you get a lower-triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51dac73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bfbc7",
   "metadata": {},
   "source": [
    "general mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9732c242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_mask_matrix(n : int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a mask matrix of size n x n where the lower triangular part (including the diagonal) is filled with 1s\n",
    "    and the upper triangular part is filled with 0s.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the matrix. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The resulting mask matrix.\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones((n, n), dtype=torch.float32), diagonal=0)\n",
    "    return mask\n",
    "\n",
    "print(build_mask_matrix(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703bba3",
   "metadata": {},
   "source": [
    "real masking matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da723d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def build_real_mask_matrix(n : int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a real mask matrix of size n x n where the lower triangular part (including the diagonal) is filled with 0s\n",
    "    and the upper triangular part is filled with -inf.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the matrix. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The resulting real mask matrix.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((n, n), dtype=torch.float32) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "print(build_real_mask_matrix(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f0353",
   "metadata": {},
   "source": [
    "# Residual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78604db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.module(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174e96f",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bcb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92bf9e",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1647bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = LinearLayer(embed_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.fc2 = LinearLayer(hidden_dim, embed_dim)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1d0f09",
   "metadata": {},
   "source": [
    "# Multi-Headed Masked Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c51e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedMaskedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Project queries, keys, values\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores + mask.unsqueeze(0).unsqueeze(1)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0daf8b",
   "metadata": {},
   "source": [
    "# Full Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba32e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-Norm GPT-style block using your components:\n",
    "      y = x + Attn(LN(x), causal_mask)\n",
    "      z = x + MLP(LN(y))\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadedMaskedAttention(embed_dim, num_heads)\n",
    "        self.drop_attn = nn.Dropout(dropout)\n",
    "\n",
    "        hidden = int(mlp_ratio * embed_dim)\n",
    "        self.ln2 = LayerNorm(embed_dim)\n",
    "        self.mlp = FeedForward(embed_dim, hidden, dropout=dropout)\n",
    "        # We can use your ResidualLayer cleanly around the MLP path\n",
    "        self.mlp_residual = ResidualLayer(self.mlp)\n",
    "\n",
    "    @staticmethod\n",
    "    def _causal_mask(T: int, device, dtype):\n",
    "        # 0 on allowed (<= i), -inf on forbidden (> i)\n",
    "        mask = torch.full((T, T), float('-inf'), device=device, dtype=dtype)\n",
    "        mask = torch.triu(mask, diagonal=1)  # upper triangle = -inf; diagonal/lower = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        B, T, D = x.shape\n",
    "        mask = self._causal_mask(T, x.device, x.dtype)  # [T, T]\n",
    "\n",
    "        # Attention block: pre-norm + residual (manual to pass mask)\n",
    "        attn_out = self.attn(self.ln1(x), mask=mask)     # [B, T, D]\n",
    "        x = x + self.drop_attn(attn_out)\n",
    "\n",
    "        # MLP block: pre-norm + residual (via your ResidualLayer)\n",
    "        x = self.mlp_residual(self.ln2(x))               # [B, T, D]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d01b0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hyperparameters according to common design choices and store them in UPPER case variables\n",
    "EMBED_DIM = 16\n",
    "NUM_HEADS = 4\n",
    "MLP_RATIO = 4.0\n",
    "DROPOUT = 0.1\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "VOCAB_SIZE = tokenizer.n_vocab\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e5c4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers: int,\n",
    "                 embed_dim: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: float = MLP_RATIO,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 num_embeddings: int = VOCAB_SIZE,\n",
    "                 max_seq_len: int = 2048\n",
    "                 ):  # Example vocab size for text models\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self._final_layer = nn.Linear(embed_dim, num_embeddings)\n",
    "        \n",
    "        # Layer normalization after final block\n",
    "        self.norm = LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len) - integer token IDs\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Get token embeddings\n",
    "        tok_emb = self.token_embedding(x)  # (B, T, embed_dim)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        positions = torch.arange(T, device=x.device)  # (T,)\n",
    "        pos_emb = self.position_embedding(positions)  # (T, embed_dim)\n",
    "\n",
    "        # Combine token and position embeddings\n",
    "        x = self.dropout(tok_emb + pos_emb)  # (B, T, embed_dim)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Final layer norm\n",
    "        return self._final_layer(self.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cba18",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f117b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Input data - should be INTEGER token IDs, not random floats\n",
    "x = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))  # Random token IDs\n",
    "\n",
    "# Instantiate the model\n",
    "GPT = TransformerDecoder(num_layers=2)\n",
    "\n",
    "# Forward pass\n",
    "out = GPT(x)\n",
    "print(out.shape)  # Should be [BATCH_SIZE, SEQ_LEN, VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d98e8",
   "metadata": {},
   "source": [
    "# Mock Training\n",
    "\n",
    "In the following section, given a simple text, we run a mock training.\n",
    "\n",
    "Steps:\n",
    "- generate mock text\n",
    "- tokenize and add positional embeddings\n",
    "- split in batches\n",
    "- construct a model\n",
    "- define train schema\n",
    "- train and measure training time and total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aa3bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read war_and_peace.txt \n",
    "with open(\"war_and_peace.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mock_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65404082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 106761 sequences of length 8\n",
      "Input shape: torch.Size([106761, 8])\n",
      "Target shape: torch.Size([106761, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAverage Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/epochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, epochs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Reshape for CrossEntropyLoss\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# outputs: (batch_size * seq_len, vocab_size)\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# labels: (batch_size * seq_len,)\u001b[39;00m\n\u001b[32m     58\u001b[39m loss = criterion(\n\u001b[32m     59\u001b[39m     outputs.view(-\u001b[32m1\u001b[39m, vocab_size),  \u001b[38;5;66;03m# Use vocab_size instead of outputs.size(-1)\u001b[39;00m\n\u001b[32m     60\u001b[39m     batch_labels.view(-\u001b[32m1\u001b[39m)\n\u001b[32m     61\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m optimizer.step()\n\u001b[32m     66\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ruggeri Gabriele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ruggeri Gabriele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ruggeri Gabriele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenized_text = tokenizer.encode(mock_text)\n",
    "vocab_size = VOCAB_SIZE\n",
    "\n",
    "# Chunk tokens into sequences of SEQ_LEN\n",
    "def create_sequences(tokens, seq_len = SEQ_LEN):\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokens) - seq_len, seq_len):\n",
    "        seq = tokens[i:i + seq_len + 1]  # +1 for target\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = create_sequences(tokenized_text, SEQ_LEN)\n",
    "print(f\"Created {len(sequences)} sequences of length {SEQ_LEN}\")\n",
    "\n",
    "# Create input-target pairs\n",
    "inputs = []\n",
    "targets = []\n",
    "for seq in sequences:\n",
    "    inputs.append(seq[:-1])   # All but last token\n",
    "    targets.append(seq[1:])   # All but first token\n",
    "\n",
    "# Convert to tensors\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)   # (num_sequences, SEQ_LEN)\n",
    "targets = torch.tensor(targets, dtype=torch.long) # (num_sequences, SEQ_LEN)\n",
    "\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Construct model\n",
    "model = TransformerDecoder(num_layers=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define train function\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            # batch_data shape: (batch_size, seq_len)\n",
    "            # batch_labels shape: (batch_size, seq_len)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - model handles embedding internally\n",
    "            outputs = model(batch_data)  # (batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            # Reshape for CrossEntropyLoss\n",
    "            # outputs: (batch_size * seq_len, vocab_size)\n",
    "            # labels: (batch_size * seq_len,)\n",
    "            loss = criterion(\n",
    "                outputs.view(-1, vocab_size),  # Use vocab_size instead of outputs.size(-1)\n",
    "                batch_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        total_loss += epoch_loss\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Average Loss: {total_loss/epochs:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
